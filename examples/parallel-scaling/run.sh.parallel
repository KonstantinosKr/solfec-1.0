#!/bin/bash
#
# use like:
#   sbatch -J jobname run.sh
#
# Don't restart if failing:
#SBATCH --no-requeue
# Whole node to our selves:
#SBATCH --exclusive
# Number of processes per node:
#SBATCH --ntasks-per-node=36
# Total number of processes:
#SBATCH -n 36
# Number of nodes:
#SBATCH -N 1
# Time required: Days-hours:mins:secs
#SBATCH --time 3-00:00:00
# Partition choice
#SBATCH --partition cn

# load modules
#module load icc
#module load ifort
#module load impi
#module load hdf5
#module load mkl

# print input config:
echo "sbatch_script: jobid = $SLURM_JOB_ID"
echo "sbatch_script: jobname = $SLURM_JOB_NAME"
echo "sbatch_script: nodes = $SLURM_NODELIST"
echo "sbatch_script: n_processes = $SLURM_NPROCS"
echo "sbatch_script: cwd = `pwd`"
echo "sbatch_script: started mpirun `date`:"

# command:
mpirun solfec-mpi input-file.py
echo "sbatch_script: ended mpirun: `date`"

# move the logfile:
set -x
mv slurm-$SLURM_JOB_ID.out $SLURM_JOB_NAME.out
