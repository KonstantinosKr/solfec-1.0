Dear POP users,

There are two examples in this directory:

1. Array of cubes - this example is meant to illustrate a simple
  finite element based multibody structural type problem, where
  a stack of cubes is subject to a sine sweep acceleration signal.

2. Rotating drum - this example is mean to illustrate a simple
  granular type problem where pseudo-rigid (or rigid) ellipsoids
  are mixed inside of a rotating drum.

How to run these examples?
==========================

Both examples have command line arguments. Because in recent months I have lost access to an HPC cluster
I was using - and only regained it today (16/10/2017) - I was only able to set up these examples on my
laptop without so testing within a HPC setting. Hence, the defaults for the command line arguments
are not representing problems that are large enough for cluster runs. Below I will provide the best
advice I can right now - while in the coming weeks I will be re-setting my HPC environment and testing
these examples on larger scale runs. I may tune those examples as a result. Should this happen I will
let you know - a new version would be available via Solfec's GitHub repository update.

OK, back to the command line arguments. You can run:

solfec inp/devel/parallel-scaling/array-of-cubes.py -h

and get:

------------------------------------------------------------------------
Array of cubes excitation parameters:
------------------------------------------------------------------------
-M number --> array edge size (default: 5)
-N number --> cube mesh edge size (default: 1)
-kifo name --> kinematics in {TL, BC, PR, RG} (default: BC)
               where: TL -- Total Lagrangian FEM
                      BC -- Body Co-rotational FEM
                      PR -- Pseudo-rigid
                      RG -- Rigid
-solv name --> solver in {NS, GS} (default: NS)
               where: NS -- Projected Newton solver
               where: GS -- Gauss-Seidel solver
-outi number --> output interval (default: 0.003)
-weak --> enable weak scaling test (default: OFF)
          in this mode a constant MxMxM array size
          per MPI rank is approximately maintained
-step number --> time step (default: 0.0005)
-stop number --> duration (default: 5)
-xdmf --> export XDMF in READ mode (default: OFF)
-help or -h --> show this help and exit

NOTE: because the output path depends on the input parameters
      use the same parameters to access results in READ mode, e.g.
      solfec -v path/to/array-of-cubes.py {same parameters}, or
      use the output directory as an input path instead, e.g.
      solfec -v path/to/results/directory
------------------------------------------------------------------------

The key parameters to make the problem larger are N and M; N produces a larger array (more bodies);
M make per-body meshes denser (don't overdo it -- 2 to 8 should do -- if you really want to test multi-body
behaviour -- otherwise calculation time may be dominated by FEM processing and solfec has not been optimized
for fast single-body FE calculations);

so for example

solfec inp/devel/parallel-scaling/array-of-cubes.py -N 20 -M 2

will produce a relatively large problem for a single cluster node; You may want to test it to see what
size is good in terms of keeping the node busy.

If you would like a shorter analysis use the -stop option, e.g.

solfec inp/devel/parallel-scaling/array-of-cubes.py -N 20 -M 2 -stop 1.0

will cut the calculation time 5-fold compared to the default;
 
You can use the -weak flag if you would like to test weak scaling; Solfec will try to keep the problem size,
per MPI-rank, constant with the growing number of MPI ranks;

=== @@@ === Output directores === @@@ ===

Now, every time you run this example, there will be an output directory generated in:

out/array-of-cubes/DIRNAME

where DIRNAME depends on the command line switches and on the number of MPI ranks;

You can always view the results graphically (if you compiled Solfec with OpenGL support) by typing:

solfec -v out/array-of-cubes/DIRNAME


to see what the example is doing; You will need to HIDE the outer boundaries which are obscuring the
box array; Press 'h' and select the drum contour and then right-click to hide bodies; Press 'ESC' to stop hiding;

==== @@@ ==== Scaling plots === @@@ ===

You can also generate scaling plots using a Python script:

python inp/devel/parallel-scaling/scaling-plots.py out/array-of-cubes

This will generate files:

RUNTIMES.png - with collective comparison of scalling of all runs (suitably combining runs with same parameters, execpt MPI rank count);

and

TIMINGS_DIRNAME.png - which provides more detailed timings of computational stages per individual set of runs (with same parameters);

To test this, just quickly run:

mpirun -np 1 solfec-mpi inp/devel/parallel-scaling/array-of-cubes.py -stop 0.1
mpirun -np 2 solfec-mpi inp/devel/parallel-scaling/array-of-cubes.py -stop 0.1
mpirun -np 4 solfec-mpi inp/devel/parallel-scaling/array-of-cubes.py -stop 0.1
...

followed by:

python inp/devel/parallel-scaling/scaling-plots.py out/array-of-cubes

and then view the contents of out/array-of-cubes in the search for *.png files;

=== @@@ === The rotating drum example === @@@ ===

This example also have command line params:

solfec inp/devel/parallel-scaling/rotating-drum.py -h

resulting in:

------------------------------------------------------------------------
Rotating drum with ellipsoidal or sphereical particles:
------------------------------------------------------------------------
-npar number --> number of particles (default: 100)
-kifo name --> kinematics in {FE, PR, RG} (default: PR)
               where: FE -- Finite Element
                      PR -- Pseudo-rigid
                      RG -- Rigid
-solv name --> solver in {NS, GS} (default: NS)
               where: NS -- Projected Newton solver
               where: GS -- Gauss-Seidel solver
-outi number --> output interval (default: 0.03)
-weak --> enable weak scaling test (default: OFF)
          in this mode a "-npar" particles per
          MPI rank is approximately maintained
-step number --> time step (default: 0.001)
-stop number --> duration (default: 10)
-fric number --> friction coefficient (default 0.3)
-angv number --> drum angular velocity [rad/s] (default 1)
-shps number --> use spherical particles (default OFF)
-xdmf --> export XDMF in READ mode (-kifo FE; default: OFF)
-help or -h --> show this help and exit

FYI: smaller time step may be appropriate for larger perticle numbers
     since the size of individual particles gets proportionally smaller

NOTE: because the output path depends on the input parameters
      use the same parameters to access results in READ mode, e.g.
      solfec -v path/to/rotating-drum.py {same parameters}, or
      use the output directory as an input path instead, e.g.
      solfec -v path/to/results/directory
------------------------------------------------------------------------

The key parameter here is -npar which determines the number of particles in the simulation;
You will need to go into tens of thousands to saturate a cluster node with 24 hardware cores;

This example is mostly meant to be used with RIGID and PSEUDO-RIGID kinematic models; In this case
the particle shape is analytical -- rigid or deforming only linearly in the pseudo-rigid case;
Finite element ellipsoidal particles are also supported - and you can try:) (may be unstable)

XXX --> The more particles you use the smaller they become; It may be practically necessary to use
smaller then the default 1 rad/s rotational speed of he drum in case of many particles;

Just run the simple serial case first:

solfec inp/devel/parallel-scaling/rotating-drum.py 

and then view the results:

solfec -v inp/devel/parallel-scaling/rotating-drum.py 

to see what it is doing; You will need to HIDE the outer drum panels in order to see the particles;
Press 'h' and select the drum contour and then right-click to hide bodies; Press 'ESC' to stop hiding;

There are two stages to the simulation here: first the bodies are injected in to the drum and then
the rotation begins; You can still use the '-stop' parameter to save computational time - but don't
overdo it - or you will not be able to simulate both stages; -stop 2.0 should still work; you can
try shorter - to save computational time;

For larger numbers of particles (-npar), as mentioned, you may need to use -angv < 1.0 in order to
get stable runs; Test with default and see whether the results seem OK; You can than halve it and
then halve again, etc.

Plotting works, in an analogous way, for this example as well;

Good luck and thank you for your interest in testing Solfec! - thank you Nick:)

Do ask for help in case of any difficulties;

Tomek

PS. You may want to try Solfec compiled with Zoltan as load balancer first;
    And then also try it compiled with the DYNLB load balancer;
